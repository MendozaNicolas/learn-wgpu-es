<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Introducción a los Compute Pipelines | Learn Wgpu - Español</title>
    <meta name="generator" content="VuePress 1.9.10">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/learn-wgpu-es/assets/css/0.styles.54a9523f.css" as="style"><link rel="preload" href="/learn-wgpu-es/assets/js/app.fb31abca.js" as="script"><link rel="preload" href="/learn-wgpu-es/assets/js/2.1022e2be.js" as="script"><link rel="preload" href="/learn-wgpu-es/assets/js/1.4766453e.js" as="script"><link rel="preload" href="/learn-wgpu-es/assets/js/43.4f83f7e5.js" as="script"><link rel="preload" href="/learn-wgpu-es/assets/js/23.247ad87d.js" as="script"><link rel="preload" href="/learn-wgpu-es/assets/js/22.5d980ee9.js" as="script"><link rel="prefetch" href="/learn-wgpu-es/assets/js/10.85d1b333.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/11.3c198606.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/12.97c631a2.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/13.8eb85f08.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/14.f56f5783.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/15.f146080a.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/16.b78eff55.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/17.ac1b915c.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/18.d64e2c10.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/19.b80b7231.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/20.5525796a.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/21.d2868a6a.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/24.086d80a7.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/25.0ba438e3.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/26.aed3aec3.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/27.b4a1c2ae.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/28.2764cb5c.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/29.58f67d16.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/3.210ed7b2.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/30.c5e766e5.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/31.5a6c1c03.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/32.b00abf9b.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/33.c2bc481b.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/34.f2bc04d8.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/35.58c2c8d8.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/36.d4783bfb.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/37.0f259387.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/38.3c99ab64.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/39.84e651b2.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/4.db40dd96.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/40.56d1e232.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/41.f90ff503.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/42.7911692d.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/44.943c009e.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/45.0ab6ac73.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/46.c4185287.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/47.777cc2bf.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/48.96be8e13.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/49.aa04b1d6.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/5.6fdb9071.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/50.4022d2a0.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/51.3c411e45.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/52.2918a592.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/53.ee6c7c86.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/54.ec95ed52.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/55.da436f39.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/56.a7464507.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/57.38978bd1.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/58.f03403ea.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/59.d0f0d3af.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/60.675b3161.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/61.0d9a5510.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/62.cb765cf3.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/8.c8ddf2a2.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/9.28f40848.js"><link rel="prefetch" href="/learn-wgpu-es/assets/js/vendors~docsearch.ccaccf87.js">
    <link rel="stylesheet" href="/learn-wgpu-es/assets/css/0.styles.54a9523f.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="inner"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/learn-wgpu-es/" class="home-link router-link-active"><!----> <span class="site-name">Learn Wgpu - Español</span></a> <div class="links"><!----> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div></div></div></header> <div class="sidebar-mask"></div> <div class="docs-layout"><aside class="sidebar"><!---->  <ul class="sidebar-links"><li><a href="/learn-wgpu-es/" aria-current="page" class="sidebar-link">Introducción</a></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Principiante</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/learn-wgpu-es/beginner/tutorial1-window/" class="sidebar-link">Dependencias y la ventana</a></li><li><a href="/learn-wgpu-es/beginner/tutorial2-surface/" class="sidebar-link">La Superficie</a></li><li><a href="/learn-wgpu-es/beginner/tutorial3-pipeline/" class="sidebar-link">El Pipeline</a></li><li><a href="/learn-wgpu-es/beginner/tutorial4-buffer/" class="sidebar-link">Búferes e Índices</a></li><li><a href="/learn-wgpu-es/beginner/tutorial5-textures/" class="sidebar-link">Texturas y grupos de enlace</a></li><li><a href="/learn-wgpu-es/beginner/tutorial6-uniforms/" class="sidebar-link">Búferes uniformes y una cámara 3D</a></li><li><a href="/learn-wgpu-es/beginner/tutorial7-instancing/" class="sidebar-link">Instanciación</a></li><li><a href="/learn-wgpu-es/beginner/tutorial8-depth/" class="sidebar-link">El Búfer de Profundidad</a></li><li><a href="/learn-wgpu-es/beginner/tutorial9-models/" class="sidebar-link">Carga de Modelos</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Intermedio</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/learn-wgpu-es/intermediate/tutorial10-lighting/" class="sidebar-link">Trabajando con Luces</a></li><li><a href="/learn-wgpu-es/intermediate/tutorial11-normals/" class="sidebar-link">Mapeo de Normales</a></li><li><a href="/learn-wgpu-es/intermediate/tutorial12-camera/" class="sidebar-link">Una Cámara Mejor</a></li><li><a href="/learn-wgpu-es/intermediate/tutorial13-hdr/" class="sidebar-link">Renderizado de Alto Rango Dinámico</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Compute Pipelines</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/learn-wgpu-es/compute/introduction/" aria-current="page" class="active sidebar-link">Introducción a los Compute Pipelines</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#por-que-el-computo-en-gpu-es-rapido" class="sidebar-link">Por qué el cómputo en GPU es rápido</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#¿cuando-debo-usar-compute-pipelines" class="sidebar-link">¿Cuándo debo usar compute pipelines?</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#configurar-el-dispositivo-y-la-cola" class="sidebar-link">Configurar el dispositivo y la cola</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#compute-pipelines" class="sidebar-link">Compute Pipelines</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#workgroups" class="sidebar-link">Workgroups</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#el-global-invocation-id" class="sidebar-link">El global invocation id</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#buffers" class="sidebar-link">Buffers</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#configuracion-de-bindgroup" class="sidebar-link">Configuración de Bindgroup</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#obtener-datos-fuera-de-la-gpu" class="sidebar-link">Obtener datos fuera de la GPU</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#conclusion" class="sidebar-link">Conclusión</a></li><li class="sidebar-sub-header"><a href="/learn-wgpu-es/compute/introduction/#demo" class="sidebar-link">Demo</a></li></ul></li><li><a href="/learn-wgpu-es/compute/sorting/" class="sidebar-link">Ordenamiento en la GPU</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Showcase</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Noticias</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="introduccion-a-los-compute-pipelines"><a href="#introduccion-a-los-compute-pipelines" class="header-anchor">#</a> Introducción a los Compute Pipelines</h1> <p>Los compute pipelines son una de las características más emocionantes que proporciona WebGPU.
Te permiten ejecutar cargas de trabajo de cómputo arbitrarias a velocidades solo posibles con
los masivos conteos de núcleos de GPU modernos. Puedes ejecutar modelos de aprendizaje automático en la
web, realizar manipulación de imágenes sin necesidad de configurar los pasos del pipeline de renderizado
como procesamiento de vértices y sombreado de fragmentos, procesar números masivos de
partículas, animar cientos de personajes rigged, etc.</p> <p>Hay muchos temas que podríamos cubrir, y lo que específicamente quieras usar
compute shaders podría no estar cubierto aquí, pero espero que sea suficiente
para comenzar. Además de eso, estoy intentando un nuevo formato donde incluiré menos
código boilerplate y enfocaré más en los conceptos. El código seguirá siendo
vinculado al final del artículo si te atascas con tu implementación.</p> <h2 id="por-que-el-computo-en-gpu-es-rapido"><a href="#por-que-el-computo-en-gpu-es-rapido" class="header-anchor">#</a> Por qué el cómputo en GPU es rápido</h2> <p>Las GPUs generalmente se consideran más rápidas que las CPUs, pero técnicamente no es
preciso. La velocidad de procesamiento de GPU es aproximadamente la misma que las CPUs, a veces incluso más lenta.
Según <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/" target="_blank" rel="noopener noreferrer">NVIDIA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
la mayoría de sus tarjetas modernas tienen velocidades de reloj alrededor de 2.5 GHz.
<a href="https://www.qualcomm.com/products/mobile/snapdragon/laptops-and-tablets/snapdragon-x-elite" target="_blank" rel="noopener noreferrer">Qualcomm anuncia<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
que el Snapdragon X Elite tiene velocidades de reloj de 3.4 - 4.3 Ghz.</p> <p>Entonces, ¿por qué las GPUs son tan populares para cargas de cómputo masivas?</p> <p>La respuesta es el conteo de núcleos. El Snapdragon X Elite tiene 12 núcleos. La RTX 5090 tiene
nada menos que 21760 núcleos. Esa es una diferencia de 4 órdenes de magnitud. Con algunas matemáticas
al dorso si un algoritmo tarda un segundo en ejecutar una operación en la CPU y
2 en la GPU, entonces dado 12000 elementos la CPU tardará 1000 segundos (alrededor de 16 minutos)
mientras que la GPU tardará 2 segundos (sin contar enviar datos hacia/desde la GPU y
tiempo de configuración).</p> <p>Quizás una demostración sea apropiada.</p> <iframe width="560" height="315" src="https://www.youtube.com/embed/vGWoV-8lteA?si=Sgl2Qq0CFoaGXMQa" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe> <p>Las GPUs son rápidas porque pueden hacer miles de cosas al mismo tiempo. Dicho esto,
no todos los algoritmos se benefician de aprovechar este poder de cómputo.</p> <h2 id="¿cuando-debo-usar-compute-pipelines"><a href="#¿cuando-debo-usar-compute-pipelines" class="header-anchor">#</a> ¿Cuándo debo usar compute pipelines?</h2> <p>No puedo hacer una lista completa de todas las cosas para las que podrías usar una GPU,
pero aquí hay algunas reglas generales:</p> <ul><li>Tareas que pueden ser fácilmente paralelizadas. Las GPUs no les gusta cambiar de tareas, así que si
necesitas que el cálculo use datos de operaciones anteriores, los compute shaders probablemente
serán más lentos que un enfoque basado en CPU. Si cada operación puede ejecutarse sin
conocimiento de otras operaciones, puedes obtener mucho de la GPU.</li> <li>Ya tienes los datos en la GPU. Si estás trabajando con datos de texturas o modelos,
a menudo puede ser más rápido procesarlos con un compute shader en lugar de copiar los datos
a la CPU, modificarlos, y luego enviarlos de vuelta a la GPU.</li> <li>Tienes una cantidad masiva de datos. En algún momento el tamaño de tus datos comienza a superar
el tiempo de configuración y la complejidad de usar un compute pipeline. Aún necesitarás adaptar
tu enfoque a los datos y el procesamiento que necesites hacer.</li></ul> <p>¡Ahora que hemos aclarado eso, comencemos!</p> <h2 id="configurar-el-dispositivo-y-la-cola"><a href="#configurar-el-dispositivo-y-la-cola" class="header-anchor">#</a> Configurar el dispositivo y la cola</h2> <p>Usar compute shaders requiere mucho menos código que usar un render pipeline. No
necesitamos una ventana, así que podemos obtener una instancia de WGPU, solicitar un adaptador, y solicitar
un dispositivo y cola con este código simple:</p> <div class="language-rust extra-class"><pre class="language-rust"><code>    <span class="token keyword">let</span> instance <span class="token operator">=</span> <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">Instance</span><span class="token punctuation">::</span><span class="token function">new</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token class-name">Default</span><span class="token punctuation">::</span><span class="token function">default</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">let</span> adapter <span class="token operator">=</span> instance<span class="token punctuation">.</span><span class="token function">request_adapter</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token class-name">Default</span><span class="token punctuation">::</span><span class="token function">default</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token keyword">await</span><span class="token punctuation">.</span><span class="token function">unwrap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">let</span> <span class="token punctuation">(</span>device<span class="token punctuation">,</span> queue<span class="token punctuation">)</span> <span class="token operator">=</span> adapter<span class="token punctuation">.</span><span class="token function">request_device</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token class-name">Default</span><span class="token punctuation">::</span><span class="token function">default</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token keyword">await</span><span class="token punctuation">.</span><span class="token function">unwrap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><div class="note"><p>Estoy usando <a href="https://docs.rs/pollster" target="_blank" rel="noopener noreferrer">pollster<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> para manejar <code>async</code> en el código nativo en
estos ejemplos. Puedes usar cualquier implementación de <code>async</code> que prefieras. También estoy
usando <a href="https://docs.rs/anyhow" target="_blank" rel="noopener noreferrer">anyhow<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> para manejo de errores, y <a href="https://docs.rs/flume" target="_blank" rel="noopener noreferrer">flume<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
para su implementación de canal <code>async</code>.</p></div> <p>Si quieres más información sobre estas llamadas y los posibles argumentos que puedes pasar
a ellas, revisa <a href="/learn-wgpu-es/beginner/tutorial2-surface/">la guía de renderizado</a>.</p> <p>Ahora que tenemos un dispositivo para comunicarnos con la GPU, comencemos a hablar sobre cómo configurar
un compute pipeline.</p> <h2 id="compute-pipelines"><a href="#compute-pipelines" class="header-anchor">#</a> Compute Pipelines</h2> <p>Los compute pipelines son mucho más simples de configurar que los render pipelines. No tenemos que configurar
el pipeline de vértices tradicional. ¡Echa un vistazo!</p> <div class="language-rust extra-class"><pre class="language-rust"><code>    <span class="token keyword">let</span> shader <span class="token operator">=</span> device<span class="token punctuation">.</span><span class="token function">create_shader_module</span><span class="token punctuation">(</span><span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token macro property">include_wgsl!</span><span class="token punctuation">(</span><span class="token string">&quot;introduction.wgsl&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token keyword">let</span> pipeline <span class="token operator">=</span> device<span class="token punctuation">.</span><span class="token function">create_compute_pipeline</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">ComputePipelineDescriptor</span> <span class="token punctuation">{</span>
        label<span class="token punctuation">:</span> <span class="token class-name">Some</span><span class="token punctuation">(</span><span class="token string">&quot;Introduction Compute Pipeline&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        layout<span class="token punctuation">:</span> <span class="token class-name">None</span><span class="token punctuation">,</span>
        module<span class="token punctuation">:</span> <span class="token operator">&amp;</span>shader<span class="token punctuation">,</span>
        entry_point<span class="token punctuation">:</span> <span class="token class-name">None</span><span class="token punctuation">,</span>
        compilation_options<span class="token punctuation">:</span> <span class="token class-name">Default</span><span class="token punctuation">::</span><span class="token function">default</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        cache<span class="token punctuation">:</span> <span class="token class-name">Default</span><span class="token punctuation">::</span><span class="token function">default</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><p>Estoy usando los valores por defecto para todo aquí excepto la <code>label</code> y el <code>module</code> del shader
que contiene el código del shader real. No estoy especificando un <code>layout</code> de bind group, lo que significa
que wgpu usará el código del shader para derivar uno. No proporciono un <code>entry_point</code> ya que WGPU
seleccionará una función con etiqueta <code>@compute</code> si hay solo una en el archivo.</p> <p>El código del shader para este ejemplo también es simple:</p> <div class="language-wgsl extra-class"><pre class="language-wgsl"><code><span class="token comment">// A read-only storage buffer that stores and array of unsigned 32bit integers</span>
<span class="token punctuation">@</span><span class="token attributes attr-name">group</span><span class="token punctuation">(</span><span class="token int-literal number">0</span><span class="token punctuation">)</span> <span class="token punctuation">@</span><span class="token attributes attr-name">binding</span><span class="token punctuation">(</span><span class="token int-literal number">0</span><span class="token punctuation">)</span> <span class="token keyword">var</span><span class="token punctuation">&lt;</span><span class="token keyword">storage</span><span class="token punctuation">,</span> read<span class="token punctuation">&gt;</span> input<span class="token punctuation">:</span> <span class="token builtin">array</span><span class="token punctuation">&lt;</span><span class="token builtin">u32</span><span class="token punctuation">&gt;</span><span class="token punctuation">;</span>
<span class="token comment">// This storage buffer can be read from and written to</span>
<span class="token punctuation">@</span><span class="token attributes attr-name">group</span><span class="token punctuation">(</span><span class="token int-literal number">0</span><span class="token punctuation">)</span> <span class="token punctuation">@</span><span class="token attributes attr-name">binding</span><span class="token punctuation">(</span><span class="token int-literal number">1</span><span class="token punctuation">)</span> <span class="token keyword">var</span><span class="token punctuation">&lt;</span><span class="token keyword">storage</span><span class="token punctuation">,</span> read_write<span class="token punctuation">&gt;</span> output<span class="token punctuation">:</span> <span class="token builtin">array</span><span class="token punctuation">&lt;</span><span class="token builtin">u32</span><span class="token punctuation">&gt;</span><span class="token punctuation">;</span>

<span class="token comment">// Tells wgpu that this function is a valid compute pipeline entry_point</span>
<span class="token punctuation">@</span><span class="token attributes attr-name">compute</span>
<span class="token comment">// Specifies the &quot;dimension&quot; of this work group</span>
<span class="token punctuation">@</span><span class="token attributes attr-name">workgroup_size</span><span class="token punctuation">(</span><span class="token int-literal number">64</span><span class="token punctuation">)</span>
<span class="token keyword">fn</span> <span class="token functions function">main</span><span class="token punctuation">(</span>
    <span class="token comment">// global_invocation_id specifies our position in the invocation grid</span>
    <span class="token punctuation">@</span><span class="token builtin-attribute"><span class="token attribute attr-name">builtin</span><span class="token punctuation">(</span><span class="token built-in-values attr-value">global_invocation_id</span><span class="token punctuation">)</span></span> global_invocation_id<span class="token punctuation">:</span> <span class="token builtin">vec3</span><span class="token punctuation">&lt;</span><span class="token builtin">u32</span><span class="token punctuation">&gt;</span>
<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">let</span> index <span class="token operator">=</span> global_invocation_id<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">let</span> total <span class="token operator">=</span> <span class="token function-calls function">arrayLength</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>input<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// workgroup_size may not be a multiple of the array size so</span>
    <span class="token comment">// we need to exit out a thread that would index out of bounds.</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>index <span class="token operator">&gt;=</span> total<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">return</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// a simple copy operation</span>
    output<span class="token punctuation">[</span>global_invocation_id<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">[</span>global_invocation_id<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre></div><p>Este shader es muy simple. Todo lo que hace es copiar el contenido de un buffer a otro.
La única cosa que siento que necesita explicación es el concepto de workgroups y <code>workgroup_size</code>.</p> <h2 id="workgroups"><a href="#workgroups" class="header-anchor">#</a> Workgroups</h2> <p>Si bien las GPUs prefieren que cada hilo pueda procesar ciegamente su trabajo, los problemas reales
requieren cierta cantidad de sincronización. Los compute shaders logran esto a través de workgroups.</p> <p>Un workgroup es un grupo de <code>X * Y * Z</code> hilos que comparten información sobre una tarea.
Definimos el tamaño de este workgroup usando la bandera <code>workgroup_size</code>. Vimos una
versión abreviada de eso arriba, pero aquí está la versión completa:</p> <div class="language-wgsl extra-class"><pre class="language-wgsl"><code><span class="token punctuation">@</span><span class="token attributes attr-name">workgroup_size</span><span class="token punctuation">(</span><span class="token int-literal number">64</span><span class="token punctuation">,</span> <span class="token int-literal number">1</span><span class="token punctuation">,</span> <span class="token int-literal number">1</span><span class="token punctuation">)</span>
</code></pre></div><p>Esto significa que nuestro compute shader creará workgroups con <code>64 * 1 * 1</code> hilos que se simplifica
a solo 64 hilos por workgroup. Si en su lugar usáramos:</p> <div class="language-wgsl extra-class"><pre class="language-wgsl"><code><span class="token punctuation">@</span><span class="token attributes attr-name">workgroup_size</span><span class="token punctuation">(</span><span class="token int-literal number">64</span><span class="token punctuation">,</span> <span class="token int-literal number">64</span><span class="token punctuation">,</span> <span class="token int-literal number">1</span><span class="token punctuation">)</span>
</code></pre></div><p>Obtendríamos <code>64 * 64 * 1</code> hilos, u 4096 hilos por workgroup.</p> <p>El tamaño máximo de workgroup soportado puede variar dependiendo de tu dispositivo, pero la especificación de WebGPU garantiza
lo siguiente:</p> <ul><li>Un tamaño máximo de workgroup X de 256</li> <li>Un tamaño máximo de workgroup Y de 256</li> <li>Un tamaño máximo de workgroup Z de 64</li> <li>Un tamaño total de workgroup de 256</li></ul> <p>Esto significa que es posible que no podamos usar <code>@workgroup_size(64, 64, 1)</code> pero <code>@workgroup_size(16, 16, 1)</code>
debería funcionar en la mayoría de dispositivos.</p> <div class="note"><h3 id="¿por-que-xyz"><a href="#¿por-que-xyz" class="header-anchor">#</a> ¿Por qué XYZ?</h3> <p>Una gran parte de los datos utilizados en programación de GPU vienen en arrays 2D e incluso 3D. Por esto <code>workgroup_size</code>
usa 3 dimensiones en lugar de 1 para hacer que escribir código multidimensional sea más conveniente.</p> <p>Por ejemplo, un blur en una imagen 2D se beneficiaría de un workgroup 2D para que cada hilo
coincida con un píxel en la imagen. Una implementación de marching cubes se beneficiaría de un workgroup 3D,
para que cada hilo maneje la geometría para un voxel en la cuadrícula de voxeles.</p></div> <h2 id="el-global-invocation-id"><a href="#el-global-invocation-id" class="header-anchor">#</a> El global invocation id</h2> <p>Cada hilo en un workgroup tiene un id asociado que dice a qué workgroup
pertenece el hilo. Si accedemos a esto usando el built-in <code>workgroup_id</code>.</p> <div class="language-wgsl extra-class"><pre class="language-wgsl"><code><span class="token punctuation">@</span><span class="token attributes attr-name">compute</span>
<span class="token punctuation">@</span><span class="token attributes attr-name">workgroup_size</span><span class="token punctuation">(</span><span class="token int-literal number">64</span><span class="token punctuation">)</span>
<span class="token keyword">fn</span> <span class="token functions function">main</span><span class="token punctuation">(</span>
    <span class="token punctuation">@</span><span class="token builtin-attribute"><span class="token attribute attr-name">builtin</span><span class="token punctuation">(</span><span class="token built-in-values attr-value">workgroup_id</span><span class="token punctuation">)</span></span> workgroup_id<span class="token punctuation">:</span> <span class="token builtin">vec3</span><span class="token punctuation">&lt;</span><span class="token builtin">u32</span><span class="token punctuation">&gt;</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token comment">// ...</span>
<span class="token punctuation">}</span>
</code></pre></div><p>Saber dónde estamos en el workgroup también es útil, y lo hacemos usando el
built-in <code>local_invocation_id</code>.</p> <div class="language-wgsl extra-class"><pre class="language-wgsl"><code><span class="token punctuation">@</span><span class="token attributes attr-name">compute</span>
<span class="token punctuation">@</span><span class="token attributes attr-name">workgroup_size</span><span class="token punctuation">(</span><span class="token int-literal number">64</span><span class="token punctuation">)</span>
<span class="token keyword">fn</span> <span class="token functions function">main</span><span class="token punctuation">(</span>
    <span class="token punctuation">@</span><span class="token builtin-attribute"><span class="token attribute attr-name">builtin</span><span class="token punctuation">(</span><span class="token built-in-values attr-value">workgroup_id</span><span class="token punctuation">)</span></span> workgroup_id<span class="token punctuation">:</span> <span class="token builtin">vec3</span><span class="token punctuation">&lt;</span><span class="token builtin">u32</span><span class="token punctuation">&gt;</span><span class="token punctuation">,</span>
    <span class="token punctuation">@</span><span class="token builtin-attribute"><span class="token attribute attr-name">builtin</span><span class="token punctuation">(</span><span class="token built-in-values attr-value">local_invocation_id</span><span class="token punctuation">)</span></span> local_invocation_id<span class="token punctuation">:</span> <span class="token builtin">vec3</span><span class="token punctuation">&lt;</span><span class="token builtin">u32</span><span class="token punctuation">&gt;</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token comment">// ...</span>
<span class="token punctuation">}</span>
</code></pre></div><p>Luego podemos calcular nuestra posición global en la cuadrícula de invocación del workgroup usando</p> <div class="language-wgsl extra-class"><pre class="language-wgsl"><code><span class="token keyword">let</span> id <span class="token operator">=</span> workgroup_id <span class="token operator">*</span> workgroup_size <span class="token operator">+</span> local_invocation_id<span class="token punctuation">;</span>
</code></pre></div><p>También podemos simplemente usar el built-in <code>global_invocation_id</code> como hicimos en el código del shader
listado arriba.</p> <h3 id="¿de-donde-viene-workgroup-id"><a href="#¿de-donde-viene-workgroup-id" class="header-anchor">#</a> ¿De dónde viene workgroup_id?</h3> <p>Cuando despachamos nuestro compute shader necesitamos especificar las dimensiones X, Y, y Z
de lo que se llama la &quot;cuadrícula de compute shader&quot;. Considera este código.</p> <div class="language-rust extra-class"><pre class="language-rust"><code>
    <span class="token punctuation">{</span>
        <span class="token comment">// We specified 64 threads per workgroup in the shader, so we need to compute how many</span>
        <span class="token comment">// workgroups we need to dispatch.</span>
        <span class="token keyword">let</span> num_dispatches <span class="token operator">=</span> input_data<span class="token punctuation">.</span><span class="token function">len</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">div_ceil</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token keyword">u32</span><span class="token punctuation">;</span>

        <span class="token keyword">let</span> <span class="token keyword">mut</span> pass <span class="token operator">=</span> encoder<span class="token punctuation">.</span><span class="token function">begin_compute_pass</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token class-name">Default</span><span class="token punctuation">::</span><span class="token function">default</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        pass<span class="token punctuation">.</span><span class="token function">set_pipeline</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>pipeline<span class="token punctuation">)</span><span class="token punctuation">;</span>
        pass<span class="token punctuation">.</span><span class="token function">set_bind_group</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>bind_group<span class="token punctuation">,</span> <span class="token operator">&amp;</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        pass<span class="token punctuation">.</span><span class="token function">dispatch_workgroups</span><span class="token punctuation">(</span>num_dispatches<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
</code></pre></div><p>En la llamada <code>pass.dispatch_workgroups()</code> usamos una cuadrícula con dimensiones <code>(num_dispatches, 1, 1)</code>
lo que significa que lanzaremos <code>num_dispatches * 1 * 1</code> workgroups. La GPU luego asigna a cada workgroup
un id con la coordenada x siendo entre 0 y <code>num_dispatches - 1</code>.</p> <p>Es importante saberlo porque si cambias el tamaño del workgroup, el <code>global_invocation_id</code> puede cambiar
lo que significa que potencialmente usarías más hilos de los que necesitas o no lo suficiente.</p> <h2 id="buffers"><a href="#buffers" class="header-anchor">#</a> Buffers</h2> <p>Aunque he cubierto buffers en la <a href="/learn-wgpu-es/beginner/tutorial4-buffer/">guía de renderizado</a>,
también los revisaré brevemente aquí. En WebGPU un buffer es memoria en la GPU que has
apartado. Esta memoria puede usarse para cualquier cosa desde datos de vértices hasta neuronas en una
red neuronal. En su mayor parte a la GPU no le importa qué datos contiene el buffer,
pero le importa cómo se usan esos datos.</p> <p>Aquí hay un ejemplo de configuración de un buffer de entrada y salida.</p> <div class="language-rust extra-class"><pre class="language-rust"><code>    <span class="token keyword">let</span> input_buffer <span class="token operator">=</span> device<span class="token punctuation">.</span><span class="token function">create_buffer_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token class-name">BufferInitDescriptor</span> <span class="token punctuation">{</span>
        label<span class="token punctuation">:</span> <span class="token class-name">Some</span><span class="token punctuation">(</span><span class="token string">&quot;input&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        contents<span class="token punctuation">:</span> <span class="token namespace">bytemuck<span class="token punctuation">::</span></span><span class="token function">cast_slice</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>input_data<span class="token punctuation">)</span><span class="token punctuation">,</span>
        usage<span class="token punctuation">:</span> <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BufferUsages</span><span class="token punctuation">::</span><span class="token constant">COPY_DST</span> <span class="token operator">|</span> <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BufferUsages</span><span class="token punctuation">::</span><span class="token constant">STORAGE</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token keyword">let</span> output_buffer <span class="token operator">=</span> device<span class="token punctuation">.</span><span class="token function">create_buffer</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BufferDescriptor</span> <span class="token punctuation">{</span>
        label<span class="token punctuation">:</span> <span class="token class-name">Some</span><span class="token punctuation">(</span><span class="token string">&quot;output&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        size<span class="token punctuation">:</span> input_buffer<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        usage<span class="token punctuation">:</span> <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BufferUsages</span><span class="token punctuation">::</span><span class="token constant">COPY_SRC</span> <span class="token operator">|</span> <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BufferUsages</span><span class="token punctuation">::</span><span class="token constant">STORAGE</span><span class="token punctuation">,</span>
        mapped_at_creation<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><p>Específicamente necesitamos el uso de <code>STORAGE</code> para nuestro buffer en este shader. Podemos
usar <code>UNIFORM</code> para algunas cosas, pero los buffers uniformes son más limitados en cuanto a
qué tamaño pueden tener y no pueden ser modificados en el shader.</p> <h2 id="configuracion-de-bindgroup"><a href="#configuracion-de-bindgroup" class="header-anchor">#</a> Configuración de Bindgroup</h2> <p>De nuevo no entraré en detalle sobre cómo definir bind groups aquí, ya que
ya lo hice en <a href="/learn-wgpu-es/beginner/tutorial5-textures/">la guía de renderizado</a>,
pero cubriré la teoría. En WebGPU un bind group describe recursos que pueden
ser usados por el shader. Estos pueden ser texturas, buffers, samplers, etc. Un
<code>BindGroupLayout</code> define cómo estos recursos se agrupan, qué etapas de shader
tienen acceso a ellos, y cómo el shader interpretará los recursos.</p> <p>Puedes especificar manualmente el <code>BindGroupLayout</code>, pero WGPU puede inferir el layout
basándose en el código del shader. Por ejemplo:</p> <div class="language-wgsl extra-class"><pre class="language-wgsl"><code><span class="token punctuation">@</span><span class="token attributes attr-name">group</span><span class="token punctuation">(</span><span class="token int-literal number">0</span><span class="token punctuation">)</span> <span class="token punctuation">@</span><span class="token attributes attr-name">binding</span><span class="token punctuation">(</span><span class="token int-literal number">0</span><span class="token punctuation">)</span> <span class="token keyword">var</span><span class="token punctuation">&lt;</span><span class="token keyword">storage</span><span class="token punctuation">,</span> read<span class="token punctuation">&gt;</span> input<span class="token punctuation">:</span> <span class="token builtin">array</span><span class="token punctuation">&lt;</span><span class="token builtin">u32</span><span class="token punctuation">&gt;</span><span class="token punctuation">;</span>
<span class="token punctuation">@</span><span class="token attributes attr-name">group</span><span class="token punctuation">(</span><span class="token int-literal number">0</span><span class="token punctuation">)</span> <span class="token punctuation">@</span><span class="token attributes attr-name">binding</span><span class="token punctuation">(</span><span class="token int-literal number">1</span><span class="token punctuation">)</span> <span class="token keyword">var</span><span class="token punctuation">&lt;</span><span class="token keyword">storage</span><span class="token punctuation">,</span> read_write<span class="token punctuation">&gt;</span> output<span class="token punctuation">:</span> <span class="token builtin">array</span><span class="token punctuation">&lt;</span><span class="token builtin">u32</span><span class="token punctuation">&gt;</span><span class="token punctuation">;</span>
</code></pre></div><p>WGPU interpreta esto como un layout con 2 entradas, un buffer de almacenamiento de solo lectura
llamado <code>input</code> en la vinculación 0, y un buffer de almacenamiento que puede ser leído y
escrito llamado <code>output</code> en la vinculación 1. Podemos crear fácilmente un bindgroup que
satisface esto con el siguiente código:</p> <div class="language-rust extra-class"><pre class="language-rust"><code>    <span class="token keyword">let</span> bind_group <span class="token operator">=</span> device<span class="token punctuation">.</span><span class="token function">create_bind_group</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BindGroupDescriptor</span> <span class="token punctuation">{</span>
        label<span class="token punctuation">:</span> <span class="token class-name">None</span><span class="token punctuation">,</span>
        layout<span class="token punctuation">:</span> <span class="token operator">&amp;</span>pipeline<span class="token punctuation">.</span><span class="token function">get_bind_group_layout</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        entries<span class="token punctuation">:</span> <span class="token operator">&amp;</span><span class="token punctuation">[</span>
            <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BindGroupEntry</span> <span class="token punctuation">{</span>
                binding<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
                resource<span class="token punctuation">:</span> input_buffer<span class="token punctuation">.</span><span class="token function">as_entire_binding</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span>
            <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BindGroupEntry</span> <span class="token punctuation">{</span>
                binding<span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
                resource<span class="token punctuation">:</span> output_buffer<span class="token punctuation">.</span><span class="token function">as_entire_binding</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><h2 id="obtener-datos-fuera-de-la-gpu"><a href="#obtener-datos-fuera-de-la-gpu" class="header-anchor">#</a> Obtener datos fuera de la GPU</h2> <p>Dependiendo de las necesidades de tu aplicación, los datos que procesas en un compute shader
pueden quedarse en la GPU ya que solo se usan para renderizado u otros compute pipelines.
Si necesitas obtener esos datos de la GPU a la CPU, o si simplemente quieres
echar un vistazo a ellos, afortunadamente hay una forma de hacerlo.</p> <p>El proceso es un poco complicado, así que veamos el código.</p> <div class="language-rust extra-class"><pre class="language-rust"><code><span class="token punctuation">{</span>
        <span class="token comment">// The mapping process is async, so we'll need to create a channel to get</span>
        <span class="token comment">// the success flag for our mapping</span>
        <span class="token keyword">let</span> <span class="token punctuation">(</span>tx<span class="token punctuation">,</span> rx<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">channel</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// We send the success or failure of our mapping via a callback</span>
        temp_buffer<span class="token punctuation">.</span><span class="token function">map_async</span><span class="token punctuation">(</span><span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">MapMode</span><span class="token punctuation">::</span><span class="token class-name">Read</span><span class="token punctuation">,</span> <span class="token punctuation">..</span><span class="token punctuation">,</span> <span class="token keyword">move</span> <span class="token closure-params"><span class="token closure-punctuation punctuation">|</span>result<span class="token closure-punctuation punctuation">|</span></span> tx<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">unwrap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// The callback we submitted to map async will only get called after the</span>
        <span class="token comment">// device is polled or the queue submitted</span>
        device<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">PollType</span><span class="token punctuation">::</span><span class="token function">wait_indefinitely</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">?</span><span class="token punctuation">;</span>

        <span class="token comment">// We check if the mapping was successful here</span>
        rx<span class="token punctuation">.</span><span class="token function">recv</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">?</span><span class="token operator">?</span><span class="token punctuation">;</span>

        <span class="token comment">// We then get the bytes that were stored in the buffer</span>
        <span class="token keyword">let</span> output_data <span class="token operator">=</span> temp_buffer<span class="token punctuation">.</span><span class="token function">get_mapped_range</span><span class="token punctuation">(</span><span class="token punctuation">..</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// Now we have the data on the CPU we can do what ever we want to with it</span>
        <span class="token macro property">assert_eq!</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>input_data<span class="token punctuation">,</span> <span class="token namespace">bytemuck<span class="token punctuation">::</span></span><span class="token function">cast_slice</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>output_data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// We need to unmap the buffer to be able to use it again</span>
    temp_buffer<span class="token punctuation">.</span><span class="token function">unmap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><p>Es posible que hayas notado que usé una variable llamada <code>temp_buffer</code> y no <code>output_buffer</code>
en el mapeo. La razón es que necesitamos que el buffer siendo mapeado tenga
el uso de <code>MAP_READ</code>. Este uso solo es compatible con el uso de <code>COPY_DST</code>, lo que significa
que no puede tener el uso de <code>STORAGE</code> ni de <code>UNIFORM</code>, lo que significa que no podemos usar el buffer
en un compute shader. Lo evitamos creando un buffer temporal al que copiamos
el <code>output_buffer</code>, y luego lo mapeamos. Aquí está el código de configuración para el <code>temp_buffer</code>:</p> <div class="language-rust extra-class"><pre class="language-rust"><code>    <span class="token keyword">let</span> temp_buffer <span class="token operator">=</span> device<span class="token punctuation">.</span><span class="token function">create_buffer</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BufferDescriptor</span> <span class="token punctuation">{</span>
        label<span class="token punctuation">:</span> <span class="token class-name">Some</span><span class="token punctuation">(</span><span class="token string">&quot;temp&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        size<span class="token punctuation">:</span> input_buffer<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        usage<span class="token punctuation">:</span> <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BufferUsages</span><span class="token punctuation">::</span><span class="token constant">COPY_DST</span> <span class="token operator">|</span> <span class="token namespace">wgpu<span class="token punctuation">::</span></span><span class="token class-name">BufferUsages</span><span class="token punctuation">::</span><span class="token constant">MAP_READ</span><span class="token punctuation">,</span>
        mapped_at_creation<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><p>Necesitamos realizar esta copia antes de enviar la cola.</p> <div class="language-rust extra-class"><pre class="language-rust"><code>    encoder<span class="token punctuation">.</span><span class="token function">copy_buffer_to_buffer</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>output_buffer<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>temp_buffer<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> output_buffer<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    queue<span class="token punctuation">.</span><span class="token function">submit</span><span class="token punctuation">(</span><span class="token punctuation">[</span>encoder<span class="token punctuation">.</span><span class="token function">finish</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><h2 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> Conclusión</h2> <p>Eso es todo. No es demasiado difícil, especialmente comparado con configurar un render pipeline. Ahora que
sabemos cómo usar un compute pipeline podemos realmente comenzar a hacer cosas más interesantes.
Esta guía no puede cubrir posiblemente todas las formas de usar compute shaders, pero planeo cubrir
algunos de los bloques de construcción centrales que necesitas para construir la mayoría de algoritmos. Después de eso puedes tomar
los conceptos y aplicarlos a tus propios proyectos!</p> <h2 id="demo"><a href="#demo" class="header-anchor">#</a> Demo</h2> <div id="wasm-example"><!----> <!----> <!----> <!----> <!----></div> <div class="auto-github-link"><a href="https://github.com/sotrh/learn-wgpu/tree/master/code/compute/src/" target="_blank" rel="noopener noreferrer">Check out the code!</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Última Actualización: </span> <span class="time">12/15/2025, 10:59:40 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/learn-wgpu-es/intermediate/tutorial13-hdr/" class="prev">
          Renderizado de Alto Rango Dinámico
        </a></span> <span class="next"><a href="/learn-wgpu-es/compute/sorting/">
          Ordenamiento en la GPU
        </a>
        →
      </span></p></div> </main></div></div><div class="global-ui"><!----></div></div>
    <script src="/learn-wgpu-es/assets/js/app.fb31abca.js" defer></script><script src="/learn-wgpu-es/assets/js/2.1022e2be.js" defer></script><script src="/learn-wgpu-es/assets/js/1.4766453e.js" defer></script><script src="/learn-wgpu-es/assets/js/43.4f83f7e5.js" defer></script><script src="/learn-wgpu-es/assets/js/23.247ad87d.js" defer></script><script src="/learn-wgpu-es/assets/js/22.5d980ee9.js" defer></script>
  </body>
</html>
